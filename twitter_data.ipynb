{"cells":[{"cell_type":"code","source":["%scala\n/* unmount if already mounted */\nval MOUNT_NAME = \"Trump100k\"\ndbutils.fs.unmount(s\"/mnt/$MOUNT_NAME\")"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["import pyspark\n\nACCESS_KEY = \"xxxxxxxxxx\"\nSECRET_KEY = \"xxxxxxxxxx\"\nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"twitteroutput\"\nMOUNT_NAME = \"Trump100k\"\n\ndbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n\ndisplay(dbutils.fs.ls(\"/mnt/%s\" % MOUNT_NAME))"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%sql\nDROP TABLE IF EXISTS TRUMP;\n\nCREATE TABLE TRUMP(\n  row_number STRING,\n  id_str STRING,\n  created_at STRING,\n  tweet_text STRING,\n  followers_count INT,\n  retweet_count INT,\n  tweet_tmstmp TIMESTAMP,\n  tweet_date DATE,\n  tweet_time STRING,\n  neg_score DECIMAL(10,2),\n  neu_score DECIMAL(10,2),\n  pos_score DECIMAL(10,2),\n  cmp_score DECIMAL(10,2)\n)\nUSING com.databricks.spark.csv\nOPTIONS (path \"dbfs:/mnt/Trump100k/Trump100k.csv\", header \"true\");"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%sql\n--check if concatenated user and tweet is distinct in the database (no dups)  \nSELECT count(distinct id_str||tweet_text) from TRUMP;"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%sql\n--check if concatenated user and tweet is always populated \nselect count(*) from TRUMP where id_str||tweet_text is null;"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%sql\n--check nulls in the table\nselect count(distinct *) from TRUMP where cmp_score is not null ;"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%sql\n--table with nulls removed\nDROP TABLE IF EXISTS TRUMP1;\ncreate table TRUMP1 as\n(\nselect * from TRUMP\nwhere cmp_score is not null \n);"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%sql\n--drop staging table and check counts match on new table\nselect count(*) from TRUMP1;"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%sql\n-- identify unique tweets i.e. the user string removed\nselect DISTINCT tweet_text||' '||neg_score||' '||neu_score||' '||pos_score||' '||cmp_score AS unique_tweet , count(id_str) as tweet_cnt \n\nFROM TRUMP1 \n\ngroup by unique_tweet\norder by tweet_cnt desc;"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%sql \n--categorical value for sentiment\n-- cmp_score ranges from -1 to 1, hence the buckets below\n-- make sure that categoricals correspond to correct range\nselect sentiment , max(cmp_score) , min(cmp_score)\nfrom\n(\n  select  cmp_score  \n      , case when cmp_score between -1 and  -0.333 then 'NEG'\n        when cmp_score between -0.333 and 0.333 then 'NEUTRAL'\n        when cmp_score between 0.333 and 1 THEN 'POS'\n        end as sentiment\n     \n\nFROM TRUMP1 ) as t1\n\ngroup by 1\norder by 1\n;"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%sql\n--create table for distinct tweets i.e. only one row per tweet (removes duplicates caused by retweets) takes the max retweet count but the min tweet time\nDROP TABLE IF EXISTS TRUMP2;\ncreate table TRUMP2 as\n(\nselect tweet_text\n      , neg_score\n      , neu_score\n      , pos_score\n      , cmp_score  \n      , case when cmp_score between -1 and  -0.333 then 'NEGATIVE'\n        when cmp_score between -0.333 and 0.333 then 'NEUTRAL'\n        when cmp_score between 0.333 and 1 THEN 'POSITIVE'\n        end as sentiment\n      , max(retweet_count) as retweets\n      , min(tweet_date)    as date\n      , min(tweet_time)    as time\n\nFROM TRUMP1 \n\ngroup by\n          tweet_text\n        , neg_score\n        , neu_score\n        , pos_score\n        , cmp_score \n        , sentiment\n)  ;"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%sql\n-- TOP 10 RETWEETED TWEETS\nselect DISTINCT tweet_text, retweets, sentiment\n\nFROM TRUMP2\n\norder by retweets desc\nlimit 10;"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%sql\n-- TWEET WITH MOST POSITIVE SENTIMENT\nselect DISTINCT t1.tweet_text , t1.cmp_score, sum(retweets)\n\nFROM TRUMP2 as t1\n\ninner join\n          (\n          select max(cmp_score) high_sentiment\n\n          FROM TRUMP2\n\n          )t2\non t2.high_sentiment = t1.cmp_score\ngroup by t1.tweet_text , t1.cmp_score\nlimit 10;"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["%sql\n-- TWEETS WITH MOST NEGATIVE SENTIMENT\nselect DISTINCT t1.tweet_text , t1.cmp_score, sum(retweets)\n\nFROM TRUMP2 as t1\n\ninner join\n          (\n          select min(cmp_score) low_sentiment\n\n          FROM TRUMP2\n\n          )t2\non t2.low_sentiment = t1.cmp_score\ngroup by t1.tweet_text , t1.cmp_score;"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["%sql\n-- distribution of sentiment of tweets about trump\nSELECT cmp_score ,sum(retweets + 1)as tweets -- 1 has been added to every retweet to account for tweets that were not retweeted\nfrom TRUMP2  where cmp_score <> 0 group by cmp_score  order by cmp_score; "],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["%sql\n--overall what type of sentiment has the most retweets\nselect sentiment ,sum(retweets+1) as tweets\nfrom trump2\ngroup by sentiment\norder by sentiment;"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%sql\n-- min and max times\nselect max(time) , min(time) from trump2;"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["%sql\n--create table for performing machine learning using spark\nDROP TABLE IF EXISTS TRUMP_SNT;\ncreate table TRUMP_SNT as\n(\nselect id_str\n      , tweet_text\n      , case when cmp_score <= 0 then 0\n        when cmp_score > 0 THEN 1\n        end as sentiment\n\n\nFROM TRUMP1 \n)  ;"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["trump = spark.table(\"TRUMP_SNT\")\nprint(trump)\ntrump.head()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n#load sentiment dataset\nsentiments_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/tables/sentiments.csv\")\n\nsentiments_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["#check positive sentiment in the dataset\nfrom pyspark.sql import functions as fn\nsentiments_df.where(fn.col('sentiment') == 1).show(5)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["#create trump df\ntrump_df = trump\ntrump_df.where(fn.col('sentiment') == 1).show(5)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["from pyspark.ml.feature import RegexTokenizer\ntokenizer = RegexTokenizer().setGaps(False)\\\n  .setPattern(\"\\\\p{L}+\")\\\n  .setInputCol(\"tweet_text\")\\\n  .setOutputCol(\"words\")"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["tweet_words_df = tokenizer.transform(trump_df)\nprint(tweet_words_df)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["#show exploded words\ntweet_words_df.show(5)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["#reweighting score to account for word occurrances\n# create stop words dataset\nimport requests\nstop_words = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words').text.split()\n\n# add rt as a stopword since it will appear on all retweets\nstop_words.append(u'rt')"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["print(stop_words)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["from pyspark.ml.feature import StopWordsRemover\nsw_filter = StopWordsRemover()\\\n  .setStopWords(stop_words)\\\n  .setCaseSensitive(False)\\\n  .setInputCol(\"words\")\\\n  .setOutputCol(\"filtered\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["from pyspark.ml.feature import CountVectorizer\n\n# we will remove words that appear in 5 docs or less\ncv = CountVectorizer(minTF=1., minDF=5., vocabSize=2**17)\\\n  .setInputCol(\"filtered\")\\\n  .setOutputCol(\"tf\")"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["#create counter vectoriser estimator\ncv_pipeline = Pipeline(stages=[tokenizer, sw_filter, cv]).fit(trump_df)\ncv_pipeline.transform(trump_df).show(5)  "],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["#new pipeline to lower the terms of documents that are very common\nfrom pyspark.ml.feature import IDF\nidf = IDF().\\\n    setInputCol('tf').\\\n    setOutputCol('tfidf')\n    \nidf_pipeline = Pipeline(stages=[cv_pipeline, idf]).fit(trump_df)\nidf_pipeline.transform(trump_df).show(5)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# create training , validation and testing sets (creating both validation and testing sets so that different models can evaluated against each other and the best can then be tested)\ntraining_df, validation_df, testing_df = trump_df.randomSplit([0.6, 0.3, 0.1], seed=0)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["#import & build Logistic Regression\nfrom pyspark.ml.classification import LogisticRegression\n\nlambda_par = 0.02\nalpha_par = 0.3\nen_lr = LogisticRegression().\\\n        setLabelCol('sentiment').\\\n        setFeaturesCol('tfidf').\\\n        setRegParam(lambda_par).\\\n        setMaxIter(100).\\\n        setElasticNetParam(alpha_par)\n        \nen_lr_pipeline = Pipeline(stages=[idf_pipeline, en_lr]).fit(training_df)        "],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["#test model accuracy\nen_lr_pipeline.transform(validation_df).select(fn.avg(fn.expr('float(prediction = sentiment)'))).show()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["#check to see which words are considered top 10 most positive/ negative in dataset\nimport pandas as pd\nvocabulary = idf_pipeline.stages[0].stages[-1].vocabulary\n\nen_weights = en_lr_pipeline.stages[-1].coefficients.toArray()\nen_coeffs_df = pd.DataFrame({'word': vocabulary, 'weight': en_weights})\n\n#negative words\nen_coeffs_df.sort_values('weight').head(10)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["#positive words\nen_coeffs_df.sort_values('weight', ascending=False).head(10)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["#experiment with different values for the lambda and alpha\nfrom pyspark.ml.tuning import ParamGridBuilder\nen_lr_estimator = Pipeline(stages=[idf_pipeline, en_lr])\ngrid = ParamGridBuilder().\\\n    addGrid(en_lr.regParam, [0., 0.01, 0.02]).\\\n    addGrid(en_lr.elasticNetParam, [0., 0.2, 0.4]).\\\n    build()\n    \ngrid    "],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["#fit models with different lambda and alapha values\nall_models = []\nfor i in range(len(grid)):\n    print(\"Fitting model {}\".format(i+1))\n    model = en_lr_estimator.fit(training_df, grid[i])\n    all_models.append(model)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["#use ROC curve to evaluate different models\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics as metric\nfrom sklearn.metrics import roc_curve, auc\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n#model output for evauation\nmodel_output1 = all_models[0].transform(testing_df)\nmodel_output2 = all_models[1].transform(testing_df)\nmodel_output3 = all_models[2].transform(testing_df)\nmodel_output4 = all_models[3].transform(testing_df)\nmodel_output5 = all_models[4].transform(testing_df)\nmodel_output6 = all_models[5].transform(testing_df)\nmodel_output7 = all_models[6].transform(testing_df)\nmodel_output8 = all_models[7].transform(testing_df)\nmodel_output9 = all_models[8].transform(testing_df)\n\n#call predicted probs and target variable\nresults1 = model_output1.select(['probability', 'sentiment'])\nresults2 = model_output2.select(['probability', 'sentiment'])\nresults3 = model_output3.select(['probability', 'sentiment'])\nresults4 = model_output4.select(['probability', 'sentiment'])\nresults5 = model_output5.select(['probability', 'sentiment'])\nresults6 = model_output6.select(['probability', 'sentiment'])\nresults7 = model_output7.select(['probability', 'sentiment'])\nresults8 = model_output8.select(['probability', 'sentiment'])\nresults9 = model_output9.select(['probability', 'sentiment'])\n\n## prepare score-label set\n# Model 1\nresults_collect1 = results1.collect()\nresults_list1 = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect1]\nscoreAndLabels1 = sc.parallelize(results_list1) \nmetrics1 = metric(scoreAndLabels1)\nfpr1 = dict()\ntpr1 = dict()\nroc_auc1 = dict() \ny_test1 = [i[1] for i in results_list1]\ny_score1 = [i[0] for i in results_list1] \nfpr1, tpr1, _ = roc_curve(y_test1, y_score1)\nroc_auc1 = auc(fpr1, tpr1)\n\n\n# Model 2\nresults_collect2 = results2.collect()\nresults_list2 = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect2]\nscoreAndLabels2 = sc.parallelize(results_list2) \nmetrics2 = metric(scoreAndLabels2)\nfpr2 = dict()\ntpr2 = dict()\nroc_auc2 = dict() \ny_test2 = [i[1] for i in results_list2]\ny_score2 = [i[0] for i in results_list2] \nfpr2, tpr2, _ = roc_curve(y_test2, y_score2)\nroc_auc2 = auc(fpr2, tpr2)\n\n\n# Model 3\nresults_collect3 = results3.collect()\nresults_list3 = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect3]\nscoreAndLabels3 = sc.parallelize(results_list3) \nmetrics3 = metric(scoreAndLabels3)\nfpr3 = dict()\ntpr3 = dict()\nroc_auc3 = dict() \ny_test3 = [i[1] for i in results_list3]\ny_score3 = [i[0] for i in results_list3] \nfpr3, tpr3, _ = roc_curve(y_test3, y_score3)\nroc_auc3 = auc(fpr3, tpr3)\n\n\n# Model 4\nresults_collect4 = results4.collect()\nresults_list4 = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect4]\nscoreAndLabels4 = sc.parallelize(results_list4) \nmetrics4 = metric(scoreAndLabels4)\nfpr4 = dict()\ntpr4 = dict()\nroc_auc4 = dict() \ny_test4 = [i[1] for i in results_list4]\ny_score4 = [i[0] for i in results_list4] \nfpr4, tpr4, _ = roc_curve(y_test4, y_score4)\nroc_auc4 = auc(fpr4, tpr4)\n\n# Model 5\nresults_collect5 = results5.collect()\nresults_list5 = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect5]\nscoreAndLabels5 = sc.parallelize(results_list5) \nmetrics5 = metric(scoreAndLabels5)\nfpr5 = dict()\ntpr5 = dict()\nroc_auc5 = dict() \ny_test5 = [i[1] for i in results_list5]\ny_score5 = [i[0] for i in results_list5] \nfpr5, tpr5, _ = roc_curve(y_test5, y_score5)\nroc_auc5 = auc(fpr5, tpr5)\n\n# Model 6\nresults_collect6 = results6.collect()\nresults_list6 = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect6]\nscoreAndLabels6 = sc.parallelize(results_list6) \nmetrics6 = metric(scoreAndLabels6)\nfpr6 = dict()\ntpr6 = dict()\nroc_auc6 = dict() \ny_test6 = [i[1] for i in results_list6]\ny_score6 = [i[0] for i in results_list6] \nfpr6, tpr6, _ = roc_curve(y_test6, y_score6)\nroc_auc6 = auc(fpr6, tpr6)\n\n# Model 7\nresults_collect7 = results7.collect()\nresults_list7 = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect7]\nscoreAndLabels7 = sc.parallelize(results_list7) \nmetrics7 = metric(scoreAndLabels7)\nfpr7 = dict()\ntpr7 = dict()\nroc_auc7 = dict() \ny_test7 = [i[1] for i in results_list7]\ny_score7 = [i[0] for i in results_list7] \nfpr7, tpr7, _ = roc_curve(y_test7, y_score7)\nroc_auc7 = auc(fpr7, tpr7)\n\n# Model 8\nresults_collect8 = results8.collect()\nresults_list8 = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect8]\nscoreAndLabels8 = sc.parallelize(results_list8) \nmetrics8 = metric(scoreAndLabels8)\nfpr8 = dict()\ntpr8 = dict()\nroc_auc8 = dict() \ny_test8 = [i[1] for i in results_list8]\ny_score8 = [i[0] for i in results_list8] \nfpr8, tpr8, _ = roc_curve(y_test8, y_score8)\nroc_auc8 = auc(fpr8, tpr8)\n\n# Model 9\nresults_collect9 = results9.collect()\nresults_list9 = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect9]\nscoreAndLabels9 = sc.parallelize(results_list9) \nmetrics9 = metric(scoreAndLabels9)\nfpr9 = dict()\ntpr9 = dict()\nroc_auc9 = dict() \ny_test9 = [i[1] for i in results_list9]\ny_score9 = [i[0] for i in results_list9] \nfpr9, tpr9, _ = roc_curve(y_test9, y_score9)\nroc_auc9 = auc(fpr9, tpr9)\n\n \nfig = plt.figure()\nplt.plot(fpr1, tpr1, linewidth= 1.5, color = '#3498db' ,label='Model 1 = %0.3f' % roc_auc1)\nplt.plot(fpr2, tpr2, linewidth= 1.5, color = '#e74c3c' ,label='Model 2 = %0.3f' % roc_auc2)\nplt.plot(fpr3, tpr3, linewidth= 1.5, color = '#2ecc71' ,label='Model 3 = %0.3f' % roc_auc3)\nplt.plot(fpr4, tpr4, linewidth= 1.5, color = '#9b59b6' ,label='Model 4 = %0.3f' % roc_auc4)\nplt.plot(fpr5, tpr5, linewidth= 1.5, color = '#FFA500' ,label='Model 5 = %0.3f' % roc_auc5)\nplt.plot(fpr6, tpr6, linewidth= 1.5, color = '#878E88' ,label='Model 6 = %0.3f' % roc_auc6)\nplt.plot(fpr7, tpr7, linewidth= 1.5, color = '#5F5AA2' ,label='Model 7 = %0.3f' % roc_auc7)\nplt.plot(fpr8, tpr8, linewidth= 1.5, color = '#EE92C2' ,label='Model 8 = %0.3f' % roc_auc8)\nplt.plot(fpr9, tpr9, linewidth= 1.5, color = '#999900' ,label='Model 9 = %0.3f' % roc_auc9)\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":39}],"metadata":{"name":"twitter_data","notebookId":2693531766206797},"nbformat":4,"nbformat_minor":0}
